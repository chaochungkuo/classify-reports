{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Data Preprocessing and Feature Engineering\"\n",
    "author:\n",
    "  Chao-Chung Kuo\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    toc-expand: 3\n",
    "    toc-location: left\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "    html-table-processing: none\n",
    "    other-links:\n",
    "      - text: Main Report\n",
    "        href: index.html\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs comprehensive data preprocessing and feature engineering following machine learning best practices. The key principle is to prevent data leakage by splitting the data early and using only training data to fit any transformers.\n",
    "\n",
    "## Critical Workflow Order\n",
    "\n",
    "1. **Data Cleaning** (Before Split) - Handle missing values, outliers, duplicates\n",
    "2. **Train-Test Split** (Early) - Split data to prevent leakage\n",
    "3. **Feature Engineering** (After Split) - Use only training data to fit transformers\n",
    "4. **Apply Transformers** - Use fitted transformers on both training and test data\n",
    "5. **Validation** - Ensure quality and no data leakage\n",
    "6. **Save Results** - Store processed data for modeling\n",
    "\n",
    "## Why This Order Matters\n",
    "\n",
    "- **Data Leakage Prevention**: Test set remains completely unseen during feature engineering\n",
    "- **Realistic Evaluation**: Model performance reflects real-world scenarios\n",
    "- **Reproducibility**: All transformations are documented and saved\n",
    "- **Best Practices**: Follows standard machine learning workflow\n",
    "\n",
    "All preprocessing steps are configurable through `config.toml` for reproducibility and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration\n",
    "import toml\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load configuration\n",
    "config = toml.load('../config.toml')\n",
    "\n",
    "# Apply visualization settings from config\n",
    "if 'visualization' in config:\n",
    "    if 'dpi' in config['visualization']:\n",
    "        plt.rcParams['figure.dpi'] = config['visualization']['dpi']\n",
    "    if 'figure_size' in config['visualization']:\n",
    "        plt.rcParams['figure.figsize'] = config['visualization']['figure_size']\n",
    "    if 'color_palette' in config['visualization']:\n",
    "        sns.set_palette(config['visualization']['color_palette'])\n",
    "\n",
    "# Display preprocessing configuration\n",
    "print(\"Preprocessing Configuration:\")\n",
    "print(f\"Imputation strategy: {config['preprocessing']['imputation_strategy']}\")\n",
    "print(f\"Outlier method: {config['preprocessing']['outlier_method']}\")\n",
    "print(f\"Normalization method: {config['preprocessing']['normalization_method']}\")\n",
    "print(f\"Test size: {config['split']['test_size']}\")\n",
    "print(f\"Random state: {config['split']['random_state']}\")\n",
    "print(f\"Stratify split: {config['split']['stratify']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Loading Data and Previous Analysis Results\n",
    "\n",
    "Before we begin preprocessing, we need to load:\n",
    "\n",
    "1. **Original Data**: The raw dataset from the first notebook\n",
    "2. **EDA Results**: Quality assessment and findings from the second notebook\n",
    "3. **Metadata**: Information about columns and data structure\n",
    "\n",
    "This ensures our preprocessing decisions are informed by the comprehensive analysis we performed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Loading data from previous notebooks...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define data directory\n",
    "data_dir = Path('../data/processed')\n",
    "\n",
    "# Load data objects from data ingestion\n",
    "with open(data_dir / '01_data_ingestion.pkl', 'rb') as f:\n",
    "    ingestion_data = pickle.load(f)\n",
    "\n",
    "# Load EDA results\n",
    "with open(data_dir / '02_eda_results.pkl', 'rb') as f:\n",
    "    eda_results = pickle.load(f)\n",
    "\n",
    "# Extract data components\n",
    "df = ingestion_data['data']['df'].copy()  # Make a copy to avoid modifying original\n",
    "feature_cols = ingestion_data['data']['feature_cols']\n",
    "sample_id_col = ingestion_data['metadata']['sample_id_col']\n",
    "label_col = ingestion_data['metadata']['label_col']\n",
    "\n",
    "# Extract metadata\n",
    "n_features = ingestion_data['metadata']['n_features']\n",
    "n_samples = ingestion_data['metadata']['n_samples']\n",
    "label_distribution = ingestion_data['metadata']['label_distribution']\n",
    "\n",
    "# Extract EDA results\n",
    "quality_metrics = eda_results['quality_metrics']\n",
    "recommendations = eda_results['recommendations']\n",
    "quality_score = eda_results['quality_score']\n",
    "correlation_matrix = eda_results['correlation_matrix']\n",
    "high_corr_pairs = eda_results['high_corr_pairs']\n",
    "outliers_by_feature = eda_results['outliers_by_feature']\n",
    "class_distribution = eda_results['class_distribution']\n",
    "feature_variances = eda_results['feature_variances']\n",
    "analysis_timestamp = eda_results['analysis_timestamp']\n",
    "\n",
    "# Summary printout\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Target variable: {label_col}\")\n",
    "print(f\"Label distribution: {label_distribution}\")\n",
    "\n",
    "# EDA Summary\n",
    "print(f\"\\nEDA Quality Score: {quality_score}/100\")\n",
    "print(f\"Missing values: {quality_metrics['missing_values_percent']:.2f}%\")\n",
    "print(f\"Outliers: {quality_metrics['outlier_percent']:.2f}%\")\n",
    "print(f\"Class balance ratio: {quality_metrics['class_balance_ratio']:.3f}\")\n",
    "\n",
    "# Show EDA recommendations\n",
    "print(f\"\\nEDA Recommendations ({len(recommendations)}):\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "# Timestamp info\n",
    "print(f\"\\nAnalysis performed at: {analysis_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Preprocessing Plan\n",
    "\n",
    "Based on our EDA findings and best practices, we'll implement the following preprocessing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Cleaning\n",
    "\n",
    "### Step 1: Missing Value Treatment\n",
    "\n",
    "**Why handle missing values before splitting?**\n",
    "\n",
    "- Missing values can affect the split process\n",
    "- Basic cleaning should be done on the entire dataset\n",
    "- No risk of data leakage for simple imputation\n",
    "\n",
    "**Imputation Strategies:**\n",
    "\n",
    "- **Mean**: Replace with feature mean (good for normally distributed data)\n",
    "- **Median**: Replace with feature median (robust to outliers)\n",
    "- **Most Frequent**: Replace with most common value (good for categorical data)\n",
    "- **KNN**: Use k-nearest neighbors to estimate missing values\n",
    "- **None**: Remove rows with missing values (if missing data is minimal)\n",
    "\n",
    "**Our Approach:**\n",
    "\n",
    "1. Analyze missing value patterns\n",
    "2. Choose appropriate imputation strategy from config\n",
    "3. Apply imputation to features and handle target separately\n",
    "4. Validate imputation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Data Cleaning\n",
    "print(\"Phase 1: Data Cleaning\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Missing Value Treatment\n",
    "print(\"Step 1: Missing Value Treatment\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Analyze missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(f\"Total missing values: {missing_data.sum()}\")\n",
    "print(f\"Overall missing percentage: {missing_data.sum() / (len(df) * len(df.columns)) * 100:.2f}%\")\n",
    "\n",
    "# Show features with missing values\n",
    "features_with_missing = missing_data[missing_data > 0]\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\nFeatures with missing values ({len(features_with_missing)}):\")\n",
    "    for feature, count in features_with_missing.items():\n",
    "        print(f\"  {feature}: {count} ({missing_percent[feature]:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n✓ No missing values found in features.\")\n",
    "\n",
    "# Check target variable for missing values\n",
    "if df[label_col].isnull().sum() > 0:\n",
    "    print(f\"\\n⚠️  Missing values in target variable: {df[label_col].isnull().sum()}\")\n",
    "    # Remove rows with missing target (we can't impute the target)\n",
    "    df = df.dropna(subset=[label_col])\n",
    "    print(f\"Removed {len(df)} rows with missing target values\")\n",
    "else:\n",
    "    print(f\"\\n✓ No missing values in target variable\")\n",
    "\n",
    "# Apply imputation strategy\n",
    "imputation_strategy = config['preprocessing']['imputation_strategy']\n",
    "print(f\"\\nApplying imputation strategy: {imputation_strategy}\")\n",
    "\n",
    "if imputation_strategy != \"none\" and len(features_with_missing) > 0:\n",
    "    # Prepare feature data for imputation\n",
    "    X_features = df[feature_cols].copy()\n",
    "    \n",
    "    if imputation_strategy == \"mean\":\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        print(\"Using mean imputation (average value for each feature)\")\n",
    "        \n",
    "    elif imputation_strategy == \"median\":\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        print(\"Using median imputation (middle value for each feature)\")\n",
    "        \n",
    "    elif imputation_strategy == \"most_frequent\":\n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "        print(\"Using most frequent imputation (most common value for each feature)\")\n",
    "        \n",
    "    elif imputation_strategy == \"knn\":\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        print(\"Using KNN imputation (k=5 nearest neighbors)\")\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_imputed = imputer.fit_transform(X_features)\n",
    "    df[feature_cols] = X_imputed\n",
    "    \n",
    "    print(\"✓ Imputation completed successfully\")\n",
    "    \n",
    "    # Save imputer for later use\n",
    "    imputer_info = {\n",
    "        'imputer': imputer,\n",
    "        'strategy': imputation_strategy,\n",
    "        'features_imputed': list(features_with_missing.index)\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(\"✓ No imputation needed (no missing values or strategy is 'none')\")\n",
    "    imputer_info = None\n",
    "\n",
    "# Validate imputation\n",
    "missing_after = df.isnull().sum().sum()\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"Missing values after imputation: {missing_after}\")\n",
    "if missing_after == 0:\n",
    "    print(\"✓ All missing values have been successfully handled\")\n",
    "else:\n",
    "    print(f\"⚠️  {missing_after} missing values remain\")\n",
    "\n",
    "print(f\"Dataset shape after missing value treatment: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Outlier Detection and Treatment\n",
    "\n",
    "**Why handle outliers before splitting?**\n",
    "\n",
    "- Outliers can affect the split process and class distribution\n",
    "- Basic outlier treatment doesn't involve complex statistics\n",
    "- No risk of data leakage for simple outlier methods\n",
    "\n",
    "**Outlier Detection Methods:**\n",
    "\n",
    "- **IQR Method**: Uses interquartile range (Q3 - Q1) to identify outliers\n",
    "- **Z-Score Method**: Uses standard deviations from the mean\n",
    "- **Isolation Forest**: Machine learning approach to detect outliers\n",
    "- **None**: Skip outlier detection\n",
    "\n",
    "**Treatment Strategies:**\n",
    "\n",
    "- **Remove**: Delete outlier samples (if percentage is low)\n",
    "- **Cap**: Limit outliers to reasonable bounds\n",
    "- **Investigate**: Flag for manual review (if percentage is high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Outlier Detection and Treatment\n",
    "print(\"\\nStep 2: Outlier Detection and Treatment\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "outlier_method = config['preprocessing']['outlier_method']\n",
    "print(f\"Using outlier detection method: {outlier_method}\")\n",
    "\n",
    "# Prepare data for outlier detection\n",
    "X_features = df[feature_cols].copy()\n",
    "\n",
    "outlier_info = {\n",
    "    'method': outlier_method,\n",
    "    'outliers_by_feature': {},\n",
    "    'total_outliers': 0,\n",
    "    'outlier_indices': set()\n",
    "}\n",
    "\n",
    "if outlier_method == \"iqr\":\n",
    "    print(\"Using IQR method for outlier detection\")\n",
    "    print(\"Outliers: values < Q1 - 1.5*IQR or > Q3 + 1.5*IQR\")\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        Q1 = X_features[col].quantile(0.25)\n",
    "        Q3 = X_features[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = ((X_features[col] < lower_bound) | (X_features[col] > upper_bound))\n",
    "        outlier_count = outliers.sum()\n",
    "        outlier_info['outliers_by_feature'][col] = outlier_count\n",
    "        outlier_info['total_outliers'] += outlier_count\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            outlier_indices = outliers[outliers].index\n",
    "            outlier_info['outlier_indices'].update(outlier_indices)\n",
    "            \n",
    "            print(f\"  {col}: {outlier_count} outliers ({outlier_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "elif outlier_method == \"zscore\":\n",
    "    print(\"Using Z-score method for outlier detection\")\n",
    "    print(\"Outliers: values with |z-score| > 3\")\n",
    "    \n",
    "    z_scores = np.abs(stats.zscore(X_features))\n",
    "    threshold = 3.0\n",
    "    \n",
    "    for i, col in enumerate(feature_cols):\n",
    "        outliers = z_scores[:, i] > threshold\n",
    "        outlier_count = outliers.sum()\n",
    "        outlier_info['outliers_by_feature'][col] = outlier_count\n",
    "        outlier_info['total_outliers'] += outlier_count\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            outlier_indices = np.where(outliers)[0]\n",
    "            outlier_info['outlier_indices'].update(outlier_indices)\n",
    "            \n",
    "            print(f\"  {col}: {outlier_count} outliers ({outlier_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "elif outlier_method == \"isolation_forest\":\n",
    "    print(\"Using Isolation Forest for outlier detection\")\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outlier_labels = iso_forest.fit_predict(X_features)\n",
    "    outliers = outlier_labels == -1\n",
    "    outlier_count = outliers.sum()\n",
    "    \n",
    "    outlier_info['total_outliers'] = outlier_count\n",
    "    outlier_info['outlier_indices'] = set(np.where(outliers)[0])\n",
    "    \n",
    "    print(f\"  Total outliers detected: {outlier_count} ({outlier_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping outlier detection (method: none)\")\n",
    "\n",
    "# Analyze outlier impact and decide treatment\n",
    "if outlier_info['total_outliers'] > 0:\n",
    "    outlier_percentage = len(outlier_info['outlier_indices']) / len(df) * 100\n",
    "    print(f\"\\nOutlier Analysis:\")\n",
    "    print(f\"Total unique samples with outliers: {len(outlier_info['outlier_indices'])}\")\n",
    "    print(f\"Percentage of samples with outliers: {outlier_percentage:.1f}%\")\n",
    "    \n",
    "    # Decide on treatment strategy\n",
    "    if outlier_percentage > 20:\n",
    "        print(\"⚠️  High outlier percentage - will cap outliers\")\n",
    "        treatment_strategy = \"cap\"\n",
    "    elif outlier_percentage > 5:\n",
    "        print(\"Moderate outlier percentage - will cap outliers\")\n",
    "        treatment_strategy = \"cap\"\n",
    "    else:\n",
    "        print(\"Low outlier percentage - will remove outliers\")\n",
    "        treatment_strategy = \"remove\"\n",
    "    \n",
    "    # Apply treatment\n",
    "    if treatment_strategy == \"remove\":\n",
    "        print(\"\\nRemoving outlier samples...\")\n",
    "        df_clean = df.drop(index=list(outlier_info['outlier_indices']))\n",
    "        removed_count = len(df) - len(df_clean)\n",
    "        df = df_clean\n",
    "        print(f\"Removed {removed_count} samples with outliers\")\n",
    "        \n",
    "    elif treatment_strategy == \"cap\":\n",
    "        print(\"\\nCapping outliers to 95th and 5th percentiles...\")\n",
    "        for col in feature_cols:\n",
    "            if outlier_info['outliers_by_feature'].get(col, 0) > 0:\n",
    "                lower_bound = X_features[col].quantile(0.05)\n",
    "                upper_bound = X_features[col].quantile(0.95)\n",
    "                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        print(\"✓ Outliers capped successfully\")\n",
    "    \n",
    "    outlier_info['treatment_strategy'] = treatment_strategy\n",
    "    outlier_info['outlier_percentage'] = outlier_percentage\n",
    "    \n",
    "else:\n",
    "    print(\"✓ No outliers detected\")\n",
    "    outlier_info['treatment_strategy'] = \"none\"\n",
    "    outlier_info['outlier_percentage'] = 0\n",
    "\n",
    "print(f\"Dataset shape after outlier treatment: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Duplicate Removal\n",
    "print(\"\\nStep 3: Duplicate Removal\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_percent = (duplicate_rows / len(df)) * 100\n",
    "\n",
    "print(f\"Duplicate rows: {duplicate_rows} ({duplicate_percent:.2f}%)\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(\"Removing duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {duplicate_rows} duplicate rows\")\n",
    "else:\n",
    "    print(\"✓ No duplicate rows found\")\n",
    "\n",
    "# Check for duplicate sample IDs (if present)\n",
    "if sample_id_col and sample_id_col in df.columns:\n",
    "    duplicate_ids = df[sample_id_col].duplicated().sum()\n",
    "    if duplicate_ids > 0:\n",
    "        print(f\"Duplicate sample IDs: {duplicate_ids}\")\n",
    "        print(\"Removing rows with duplicate sample IDs...\")\n",
    "        df = df.drop_duplicates(subset=[sample_id_col])\n",
    "        print(f\"Removed {duplicate_ids} rows with duplicate sample IDs\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate sample IDs found\")\n",
    "\n",
    "print(f\"Dataset shape after duplicate removal: {df.shape}\")\n",
    "\n",
    "# Summary of cleaning phase\n",
    "print(f\"\\nPhase 1 Summary:\")\n",
    "print(f\"Original shape: {ingestion_data['data']['df'].shape}\")\n",
    "print(f\"After cleaning: {df.shape}\")\n",
    "print(f\"Rows removed: {ingestion_data['data']['df'].shape[0] - df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Train-Test Split\n",
    "\n",
    "### Why Split Early?\n",
    "\n",
    "**Critical Principle**: We split the data BEFORE any feature engineering to prevent data leakage.\n",
    "\n",
    "**What happens if we don't split early?**\n",
    "\n",
    "- Feature selection would use information from test set\n",
    "- Normalization would be influenced by test set statistics\n",
    "- Model evaluation would be overly optimistic\n",
    "- Results wouldn't reflect real-world performance\n",
    "\n",
    "**Our Approach:**\n",
    "\n",
    "1. Split data after basic cleaning\n",
    "2. Use only training data for all feature engineering\n",
    "3. Apply fitted transformers to test data\n",
    "4. Maintain proper separation throughout\n",
    "\n",
    "### Split Strategy\n",
    "\n",
    "- **Test Size**: Percentage from config (typically 20%)\n",
    "- **Random State**: Fixed seed for reproducibility\n",
    "- **Stratification**: Maintain class distribution in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Train-Test Split\n",
    "print(\"Phase 2: Train-Test Split\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get split parameters from config\n",
    "test_size = config['split']['test_size']\n",
    "random_state = config['split']['random_state']\n",
    "stratify = config['split']['stratify']\n",
    "\n",
    "print(f\"Split parameters:\")\n",
    "print(f\"  Test size: {test_size}\")\n",
    "print(f\"  Random state: {random_state}\")\n",
    "print(f\"  Stratify: {stratify}\")\n",
    "\n",
    "# Prepare data for splitting\n",
    "X = df[feature_cols]\n",
    "y = df[label_col]\n",
    "\n",
    "print(f\"\\nData prepared for splitting:\")\n",
    "print(f\"  Features shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "\n",
    "# Perform train-test split\n",
    "if stratify:\n",
    "    print(\"\\nPerforming stratified split...\")\n",
    "    X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "        X, y, range(len(df)), \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nPerforming random split...\")\n",
    "    X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "        X, y, range(len(df)), \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "# Create train and test dataframes\n",
    "train_df = df.iloc[train_indices].copy()\n",
    "test_df = df.iloc[test_indices].copy()\n",
    "\n",
    "print(f\"\\nSplit results:\")\n",
    "print(f\"  Training set: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test set: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Verify class distribution in splits\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "train_class_dist = train_df[label_col].value_counts()\n",
    "for class_name, count in train_class_dist.items():\n",
    "    print(f\"  {class_name}: {count} ({count/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "test_class_dist = test_df[label_col].value_counts()\n",
    "for class_name, count in test_class_dist.items():\n",
    "    print(f\"  {class_name}: {count} ({count/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "# Check if stratification worked\n",
    "if stratify:\n",
    "    original_dist = df[label_col].value_counts(normalize=True)\n",
    "    train_dist = train_df[label_col].value_counts(normalize=True)\n",
    "    test_dist = test_df[label_col].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"\\nStratification check:\")\n",
    "    for class_name in original_dist.index:\n",
    "        orig_pct = original_dist[class_name] * 100\n",
    "        train_pct = train_dist[class_name] * 100\n",
    "        test_pct = test_dist[class_name] * 100\n",
    "        print(f\"  {class_name}: Original={orig_pct:.1f}%, Train={train_pct:.1f}%, Test={test_pct:.1f}%\")\n",
    "\n",
    "# Save split information\n",
    "split_info = {\n",
    "    'test_size': test_size,\n",
    "    'random_state': random_state,\n",
    "    'stratify': stratify,\n",
    "    'train_indices': train_indices,\n",
    "    'test_indices': test_indices,\n",
    "    'train_shape': train_df.shape,\n",
    "    'test_shape': test_df.shape,\n",
    "    'train_class_distribution': train_class_dist.to_dict(),\n",
    "    'test_class_distribution': test_class_dist.to_dict()\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Train-test split completed successfully\")\n",
    "print(\"⚠️  IMPORTANT: From now on, we will only use training data to fit any transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Phase 3: Feature Engineering\n",
    "\n",
    "### Critical Principle: Training Data Only\n",
    "\n",
    "**Why use only training data for feature engineering?**\n",
    "\n",
    "- **No Data Leakage**: Test set remains completely unseen\n",
    "- **Realistic Evaluation**: Model performance reflects real-world scenarios\n",
    "- **Proper ML Workflow**: Follows standard machine learning best practices\n",
    "\n",
    "**Our Approach:**\n",
    "\n",
    "1. **Fit transformers on training data only**\n",
    "2. **Transform training data** using fitted transformers\n",
    "3. **Transform test data** using the SAME fitted transformers\n",
    "4. **Never refit transformers** on test data\n",
    "\n",
    "### Feature Engineering Steps\n",
    "\n",
    "1. **Feature Selection**: Remove highly correlated and low-variance features\n",
    "2. **Feature Transformation**: Apply normalization and handle skewed distributions\n",
    "3. **Categorical Encoding**: Handle categorical variables if present\n",
    "\n",
    "### Example of Proper Workflow\n",
    "```python\n",
    "# CORRECT approach:\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training\n",
    "X_test_scaled = scaler.transform(X_test)        # Apply to test\n",
    "\n",
    "# WRONG approach (data leakage):\n",
    "scaler = StandardScaler()\n",
    "X_all_scaled = scaler.fit_transform(X_all)      # Uses test data info\n",
    "```\n",
    "\n",
    "Let's implement this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Feature Engineering\n",
    "print(\"Phase 3: Feature Engineering\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"⚠️  REMINDER: All feature engineering will use training data only!\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Step 1: Feature Selection - Correlation Removal\n",
    "print(\"Step 1: Feature Selection - Correlation Removal\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate correlation matrix using TRAINING DATA ONLY\n",
    "print(\"Calculating correlation matrix using training data only...\")\n",
    "train_correlation_matrix = train_df[feature_cols].corr()\n",
    "\n",
    "# Find highly correlated features using training data\n",
    "correlation_threshold = 0.8  # Can be made configurable\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(train_correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(train_correlation_matrix.columns)):\n",
    "        corr_val = train_correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > correlation_threshold:\n",
    "            high_corr_pairs.append((\n",
    "                train_correlation_matrix.columns[i],\n",
    "                train_correlation_matrix.columns[j],\n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "print(f\"Features with correlation > {correlation_threshold}: {len(high_corr_pairs)}\")\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly correlated feature pairs (first 10):\")\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "        print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "\n",
    "# Find features to remove based on high correlation\n",
    "high_corr_features = set()\n",
    "\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    # Keep the feature with higher variance (more information)\n",
    "    var1 = train_df[feat1].var()\n",
    "    var2 = train_df[feat2].var()\n",
    "    \n",
    "    if var1 >= var2:\n",
    "        high_corr_features.add(feat2)\n",
    "    else:\n",
    "        high_corr_features.add(feat1)\n",
    "\n",
    "print(f\"\\nFeatures to remove due to high correlation: {len(high_corr_features)}\")\n",
    "if high_corr_features:\n",
    "    print(\"Removing:\", list(high_corr_features)[:10])  # Show first 10\n",
    "\n",
    "# Remove highly correlated features\n",
    "feature_cols_clean = [col for col in feature_cols if col not in high_corr_features]\n",
    "print(f\"Features after correlation removal: {len(feature_cols_clean)}\")\n",
    "\n",
    "# Update feature columns for both training and test sets\n",
    "X_train_clean = train_df[feature_cols_clean]\n",
    "X_test_clean = test_df[feature_cols_clean]\n",
    "\n",
    "print(f\"Training features shape: {X_train_clean.shape}\")\n",
    "print(f\"Test features shape: {X_test_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Selection - Variance Selection\n",
    "print(\"\\nStep 2: Feature Selection - Variance Selection\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Remove low-variance features using TRAINING DATA ONLY\n",
    "variance_threshold = 0.01  # Can be made configurable\n",
    "variance_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "\n",
    "print(f\"Using variance threshold: {variance_threshold}\")\n",
    "print(\"Fitting variance selector on training data only...\")\n",
    "\n",
    "# Fit variance selector on training data only\n",
    "variance_selector.fit(X_train_clean)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_mask = variance_selector.get_support()\n",
    "low_variance_features = [col for col, selected in zip(feature_cols_clean, selected_features_mask) if not selected]\n",
    "\n",
    "print(f\"Low-variance features removed: {len(low_variance_features)}\")\n",
    "if low_variance_features:\n",
    "    print(\"Removed:\", low_variance_features[:10])  # Show first 10\n",
    "\n",
    "# Get final feature list\n",
    "final_features = [col for col, selected in zip(feature_cols_clean, selected_features_mask) if selected]\n",
    "print(f\"Final features after selection: {len(final_features)}\")\n",
    "\n",
    "# Apply feature selection to both datasets\n",
    "X_train_selected = X_train_clean[final_features]\n",
    "X_test_selected = test_df[final_features]  # Use original test_df to get final features\n",
    "\n",
    "print(f\"Training features after selection: {X_train_selected.shape}\")\n",
    "print(f\"Test features after selection: {X_test_selected.shape}\")\n",
    "\n",
    "# Save feature selection info\n",
    "feature_selection_info = {\n",
    "    'original_features': len(feature_cols),\n",
    "    'features_after_correlation_removal': len(feature_cols_clean),\n",
    "    'features_after_variance_selection': len(final_features),\n",
    "    'removed_correlated_features': list(high_corr_features),\n",
    "    'removed_low_variance_features': low_variance_features,\n",
    "    'final_features': final_features,\n",
    "    'correlation_threshold': correlation_threshold,\n",
    "    'variance_threshold': variance_threshold\n",
    "}\n",
    "\n",
    "print(f\"\\nFeature Selection Summary:\")\n",
    "print(f\"Original features: {feature_selection_info['original_features']}\")\n",
    "print(f\"After correlation removal: {feature_selection_info['features_after_correlation_removal']}\")\n",
    "print(f\"After variance selection: {feature_selection_info['features_after_variance_selection']}\")\n",
    "print(f\"Total features removed: {len(high_corr_features) + len(low_variance_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Transformation - Normalization\n",
    "print(\"\\nStep 3: Feature Transformation - Normalization\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "normalization_method = config['preprocessing']['normalization_method']\n",
    "print(f\"Using normalization method: {normalization_method}\")\n",
    "\n",
    "# Initialize scaler based on config\n",
    "if normalization_method == \"zscore\":\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Using Z-score normalization (mean=0, std=1)\")\n",
    "    print(\"Formula: (x - mean) / std\")\n",
    "    \n",
    "elif normalization_method == \"minmax\":\n",
    "    scaler = MinMaxScaler()\n",
    "    print(\"Using Min-Max normalization (range [0,1])\")\n",
    "    print(\"Formula: (x - min) / (max - min)\")\n",
    "    \n",
    "elif normalization_method == \"robust\":\n",
    "    scaler = RobustScaler()\n",
    "    print(\"Using Robust normalization (median=0, IQR=1)\")\n",
    "    print(\"Formula: (x - median) / IQR\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping normalization\")\n",
    "    scaler = None\n",
    "\n",
    "# Apply normalization if specified\n",
    "if scaler is not None:\n",
    "    print(\"\\nFitting scaler on training data only...\")\n",
    "    print(\"This is the critical step that prevents data leakage!\")\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    \n",
    "    # Apply the SAME fitted scaler to test data\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    print(\"✓ Normalization completed successfully\")\n",
    "    print(\"✓ Scaler fitted on training data only\")\n",
    "    print(\"✓ Same scaler applied to test data\")\n",
    "    \n",
    "    # Show sample of normalized data\n",
    "    print(\"\\nSample of normalized training data:\")\n",
    "    print(pd.DataFrame(X_train_scaled, columns=final_features).head())\n",
    "    \n",
    "    # Show scaler parameters (from training data)\n",
    "    if hasattr(scaler, 'mean_'):\n",
    "        print(f\"\\nScaler parameters (from training data):\")\n",
    "        print(f\"Mean: {scaler.mean_[:5]}...\")  # Show first 5\n",
    "        print(f\"Scale: {scaler.scale_[:5]}...\")  # Show first 5\n",
    "    \n",
    "else:\n",
    "    print(\"✓ No normalization applied\")\n",
    "    X_train_scaled = X_train_selected.values\n",
    "    X_test_scaled = X_test_selected.values\n",
    "\n",
    "# Save normalization info\n",
    "normalization_info = {\n",
    "    'method': normalization_method,\n",
    "    'scaler': scaler,\n",
    "    'features_normalized': final_features\n",
    "}\n",
    "\n",
    "print(f\"\\nNormalization Summary:\")\n",
    "print(f\"Method: {normalization_method}\")\n",
    "print(f\"Features normalized: {len(final_features)}\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Data Validation\n",
    "\n",
    "### What We're Validating\n",
    "\n",
    "1. **Data Completeness**: No missing values\n",
    "2. **Data Types**: Correct data types for each column\n",
    "3. **Value Ranges**: Features are within expected ranges\n",
    "4. **Distribution Changes**: How preprocessing affected data distributions\n",
    "5. **Data Leakage Check**: Ensure test set wasn't used for fitting\n",
    "6. **Statistical Properties**: Key statistics before and after preprocessing\n",
    "\n",
    "### Validation Checks\n",
    "\n",
    "- **Missing Values**: Ensure no missing values remain\n",
    "- **Data Types**: Verify correct data types\n",
    "- **Value Ranges**: Check for reasonable value ranges\n",
    "- **Distribution Analysis**: Compare before/after distributions\n",
    "- **Statistical Summary**: Key statistics and changes\n",
    "- **Data Leakage Verification**: Confirm proper workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Data Validation\n",
    "print(\"Phase 4: Data Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 4.1 Check for missing values\n",
    "print(\"4.1 Checking for missing values...\")\n",
    "train_missing = np.isnan(X_train_scaled).sum()\n",
    "test_missing = np.isnan(X_test_scaled).sum()\n",
    "\n",
    "if train_missing == 0 and test_missing == 0:\n",
    "    print(\"✓ No missing values found in processed data\")\n",
    "else:\n",
    "    print(f\"⚠️  Missing values found: Train={train_missing}, Test={test_missing}\")\n",
    "\n",
    "# 4.2 Check data types and value ranges\n",
    "print(\"\\n4.2 Checking data types and value ranges...\")\n",
    "\n",
    "# Check for infinite values\n",
    "train_infinite = np.isinf(X_train_scaled).sum()\n",
    "test_infinite = np.isinf(X_test_scaled).sum()\n",
    "\n",
    "if train_infinite == 0 and test_infinite == 0:\n",
    "    print(\"✓ No infinite values found\")\n",
    "else:\n",
    "    print(f\"⚠️  Infinite values found: Train={train_infinite}, Test={test_infinite}\")\n",
    "\n",
    "# Check value ranges\n",
    "print(\"\\nValue ranges (first 5 features):\")\n",
    "for i, feature in enumerate(final_features[:5]):\n",
    "    train_min, train_max = X_train_scaled[:, i].min(), X_train_scaled[:, i].max()\n",
    "    test_min, test_max = X_test_scaled[:, i].min(), X_test_scaled[:, i].max()\n",
    "    print(f\"  {feature}: Train[{train_min:.3f}, {train_max:.3f}], Test[{test_min:.3f}, {test_max:.3f}]\")\n",
    "\n",
    "# 4.3 Distribution comparison (before vs after)\n",
    "print(\"\\n4.3 Distribution analysis...\")\n",
    "\n",
    "# Create comparison plots for first 3 features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "original_train_data = train_df[final_features[:3]]\n",
    "\n",
    "for i, feature in enumerate(final_features[:3]):\n",
    "    # Before preprocessing\n",
    "    axes[0, i].hist(original_train_data[feature], bins=30, alpha=0.7, color='blue')\n",
    "    axes[0, i].set_title(f'{feature} (Before)')\n",
    "    axes[0, i].set_xlabel('Value')\n",
    "    \n",
    "    # After preprocessing\n",
    "    axes[1, i].hist(X_train_scaled[:, i], bins=30, alpha=0.7, color='green')\n",
    "    axes[1, i].set_title(f'{feature} (After)')\n",
    "    axes[1, i].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.4 Statistical summary comparison\n",
    "print(\"\\n4.4 Statistical summary comparison:\")\n",
    "print(\"Before preprocessing (first 3 features):\")\n",
    "print(original_train_data.describe())\n",
    "\n",
    "print(\"\\nAfter preprocessing (first 3 features):\")\n",
    "processed_df = pd.DataFrame(X_train_scaled[:, :3], columns=final_features[:3])\n",
    "print(processed_df.describe())\n",
    "\n",
    "# 4.5 Data leakage verification\n",
    "print(\"\\n4.5 Data leakage verification...\")\n",
    "print(\"✓ Train-test split performed before feature engineering\")\n",
    "print(\"✓ Feature selection used only training data\")\n",
    "print(\"✓ Normalization fitted on training data only\")\n",
    "print(\"✓ Test data transformed using fitted transformers\")\n",
    "print(\"✓ No information from test set influenced preprocessing\")\n",
    "\n",
    "# 4.6 Quality metrics\n",
    "print(\"\\n4.6 Quality metrics:\")\n",
    "validation_metrics = {\n",
    "    'train_shape': X_train_scaled.shape,\n",
    "    'test_shape': X_test_scaled.shape,\n",
    "    'feature_count': len(final_features),\n",
    "    'train_missing': train_missing,\n",
    "    'test_missing': test_missing,\n",
    "    'train_infinite': train_infinite,\n",
    "    'test_infinite': test_infinite\n",
    "}\n",
    "\n",
    "for metric, value in validation_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Overall validation score\n",
    "validation_score = 100\n",
    "if train_missing > 0 or test_missing > 0:\n",
    "    validation_score -= 20\n",
    "if train_infinite > 0 or test_infinite > 0:\n",
    "    validation_score -= 20\n",
    "\n",
    "print(f\"\\nValidation Score: {validation_score}/100\")\n",
    "if validation_score >= 80:\n",
    "    print(\"✓ Data validation passed\")\n",
    "else:\n",
    "    print(\"⚠️  Data validation issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Save Results\n",
    "\n",
    "### What We're Saving\n",
    "\n",
    "We'll save all the processed data and preprocessing information for use in subsequent notebooks:\n",
    "\n",
    "1. **Processed Datasets**: Clean training and test sets\n",
    "2. **Preprocessing Pipeline**: All transformers and parameters\n",
    "3. **Feature Information**: Selected features and metadata\n",
    "4. **Split Information**: Train-test split details\n",
    "5. **Preprocessing Report**: Summary of all steps applied\n",
    "\n",
    "### Why Save Everything?\n",
    "\n",
    "1. **Reproducibility**: Ensure consistent preprocessing across runs\n",
    "2. **Pipeline Continuity**: Next notebooks can use the same preprocessing\n",
    "3. **Documentation**: Record all decisions and transformations\n",
    "4. **Debugging**: Easy to trace issues back to preprocessing steps\n",
    "\n",
    "### File Structure\n",
    "\n",
    "- `processed_data.pkl`: Training and test datasets\n",
    "- `preprocessing_pipeline.pkl`: All transformers and parameters\n",
    "- `feature_info.pkl`: Feature selection and metadata\n",
    "- `preprocessing_summary.json`: Human-readable summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Save Results\n",
    "print(\"Phase 5: Save Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Save processed datasets\n",
    "processed_data = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_cols': final_features,\n",
    "    'label_col': label_col,\n",
    "    'sample_id_col': sample_id_col,\n",
    "    'train_indices': train_indices,\n",
    "    'test_indices': test_indices\n",
    "}\n",
    "\n",
    "with open(output_dir / '03_processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "print(\"✓ Processed datasets saved\")\n",
    "\n",
    "# 2. Save preprocessing pipeline\n",
    "preprocessing_pipeline = {\n",
    "    'imputer_info': imputer_info,\n",
    "    'outlier_info': outlier_info,\n",
    "    'feature_selection_info': feature_selection_info,\n",
    "    'normalization_info': normalization_info,\n",
    "    'split_info': split_info,\n",
    "    'validation_metrics': validation_metrics,\n",
    "    'validation_score': validation_score\n",
    "}\n",
    "\n",
    "with open(output_dir / '03_preprocessing_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_pipeline, f)\n",
    "print(\"✓ Preprocessing pipeline saved\")\n",
    "\n",
    "# 3. Save feature information\n",
    "feature_info = {\n",
    "    'original_features': feature_cols,\n",
    "    'final_features': final_features,\n",
    "    'removed_correlated': feature_selection_info['removed_correlated_features'],\n",
    "    'removed_low_variance': feature_selection_info['removed_low_variance_features']\n",
    "}\n",
    "\n",
    "with open(output_dir / '03_feature_info.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "print(\"✓ Feature information saved\")\n",
    "\n",
    "# 4. Create preprocessing summary\n",
    "preprocessing_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'original_shape': ingestion_data['data']['df'].shape,\n",
    "        'cleaned_shape': df.shape,\n",
    "        'train_shape': X_train_scaled.shape,\n",
    "        'test_shape': X_test_scaled.shape\n",
    "    },\n",
    "    'preprocessing_steps': {\n",
    "        'imputation': {\n",
    "            'strategy': imputation_strategy,\n",
    "            'features_imputed': len(features_with_missing) if features_with_missing is not None else 0\n",
    "        },\n",
    "        'outlier_treatment': {\n",
    "            'method': outlier_method,\n",
    "            'outliers_found': outlier_info['total_outliers'],\n",
    "            'treatment_strategy': outlier_info.get('treatment_strategy', 'none')\n",
    "        },\n",
    "        'feature_selection': {\n",
    "            'original_features': feature_selection_info['original_features'],\n",
    "            'final_features': len(final_features),\n",
    "            'correlation_removal': len(feature_selection_info['removed_correlated_features']),\n",
    "            'variance_selection': len(feature_selection_info['removed_low_variance_features'])\n",
    "        },\n",
    "        'normalization': {\n",
    "            'method': normalization_method,\n",
    "            'features_normalized': len(final_features)\n",
    "        }\n",
    "    },\n",
    "    'quality_metrics': {\n",
    "        'validation_score': validation_score,\n",
    "        'train_missing': train_missing,\n",
    "        'test_missing': test_missing,\n",
    "        'train_infinite': train_infinite,\n",
    "        'test_infinite': test_infinite\n",
    "    },\n",
    "    'split_info': {\n",
    "        'test_size': test_size,\n",
    "        'stratify': stratify,\n",
    "        'train_samples': X_train_scaled.shape[0],\n",
    "        'test_samples': X_test_scaled.shape[0]\n",
    "    },\n",
    "    'data_leakage_prevention': {\n",
    "        'split_before_feature_engineering': True,\n",
    "        'feature_selection_on_training_only': True,\n",
    "        'normalization_fitted_on_training_only': True,\n",
    "        'test_data_unseen_during_preprocessing': True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / '03_preprocessing_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_summary, f)\n",
    "print(\"✓ Preprocessing summary saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
