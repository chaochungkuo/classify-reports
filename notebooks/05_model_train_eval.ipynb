{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Model Training and Evaluation\"\n",
    "author:\n",
    "  Chao-Chung Kuo\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    toc-expand: 3\n",
    "    toc-location: left\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "    html-table-processing: none\n",
    "    other-links:\n",
    "      - text: Main Report\n",
    "        href: index.html\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "This notebook trains and evaluates multiple classification models using the preprocessed and feature-selected data. We compare different models' performance and select the best performing ones.\n",
    "\n",
    "Load the preprocessed data from previous steps. The data has already been:\n",
    "\n",
    "- Split into train/test sets\n",
    "- Preprocessed (cleaned, normalized)\n",
    "- Feature selected (using training data only)\n",
    "\n",
    "Key Features\n",
    "\n",
    "- Automatic Classification Detection: Detects binary vs multiclass automatically\n",
    "- Comprehensive Model Support: Linear, tree-based, SVM, and neural networks\n",
    "- Class Imbalance Handling: SMOTE, class weights, or no handling\n",
    "- Cross-Validation: Stratified k-fold cross-validation\n",
    "- Hyperparameter Tuning: Grid search optimization\n",
    "- Progress Tracking: Clear logging and progress indicators\n",
    "\n",
    "Models Supported\n",
    "\n",
    "- Linear: Logistic Regression\n",
    "- Tree-based: Random Forest, XGBoost, LightGBM\n",
    "- Kernel Methods: Support Vector Machine\n",
    "- Neural Networks: Multi-layer Perceptron\n",
    "\n",
    "All training is done using only the training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import toml\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import utility functions\n",
    "import sys\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sys.path.append(str(Path().resolve()))\n",
    "from model_training import (\n",
    "    detect_classification_type, handle_class_imbalance, \n",
    "    ModelFactory, train_model_with_cv, evaluate_model,\n",
    "    get_class_distribution\n",
    ")\n",
    "\n",
    "# Load configuration\n",
    "config = toml.load('../config.toml')\n",
    "\n",
    "# Display configuration\n",
    "print(\"Model Training Configuration:\")\n",
    "print(f\"Models to train: {config['modeling']['models']}\")\n",
    "print(f\"Cross-validation folds: {config['modeling']['cv_folds']}\")\n",
    "print(f\"Class imbalance handling: {config['modeling']['class_imbalance']}\")\n",
    "print(f\"Hyperparameter search method: {config['hyperparameter_tuning']['search_method']}\")\n",
    "print(f\"Parallel jobs: {config['hyperparameter_tuning']['n_jobs']}\")\n",
    "\n",
    "# Apply visualization settings from config\n",
    "if 'visualization' in config:\n",
    "    if 'dpi' in config['visualization']:\n",
    "        plt.rcParams['figure.dpi'] = config['visualization']['dpi']\n",
    "    if 'figure_size' in config['visualization']:\n",
    "        plt.rcParams['figure.figsize'] = config['visualization']['figure_size']\n",
    "    if 'color_palette' in config['visualization']:\n",
    "        sns.set_palette(config['visualization']['color_palette'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load feature-selected data from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-selected data from previous step\n",
    "data_dir = Path('../data/processed')\n",
    "with open(data_dir / '04_feature_selection.pkl', 'rb') as f:\n",
    "    feature_data = pickle.load(f)\n",
    "\n",
    "X_train = feature_data['X_train_selected']\n",
    "X_test = feature_data['X_test_selected']\n",
    "y_train = feature_data['y_train']\n",
    "y_test = feature_data['y_test']\n",
    "selected_features = feature_data['selected_features']\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "\n",
    "# Detect classification type\n",
    "classification_type, n_classes = detect_classification_type(y_train)\n",
    "print(f\"\\nClassification type: {classification_type}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Class distribution: {get_class_distribution(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle class imbalance if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance if needed\n",
    "if config['modeling']['class_imbalance'] != \"none\":\n",
    "    print(f\"\\nHandling class imbalance with strategy: {config['modeling']['class_imbalance']}\")\n",
    "    X_train_balanced, y_train_balanced = handle_class_imbalance(\n",
    "        X_train, y_train, config['modeling']['class_imbalance']\n",
    "    )\n",
    "else:\n",
    "    X_train_balanced, y_train_balanced = X_train, y_train\n",
    "    print(\"\\nNo class imbalance handling applied\")\n",
    "\n",
    "print(f\"Final training set shape: {X_train_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Factory\n",
    "\n",
    "The ModelFactory handles model creation and configuration:\n",
    "\n",
    "- Creates model instances with appropriate hyperparameters\n",
    "- Manages label encoding for models requiring numeric labels (XGBoost, LightGBM)\n",
    "- Ensures consistent preprocessing across all models\n",
    "- Configures cross-validation settings\n",
    "\n",
    "Train each model specified in config.toml:\n",
    "\n",
    "- Use cross-validation for robust performance estimation\n",
    "- Apply hyperparameter tuning via grid search\n",
    "- Handle both string and numeric labels appropriately\n",
    "- Track training progress and success/failure status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model factory and fit label encoder\n",
    "factory = ModelFactory(classification_type, n_classes, config)\n",
    "factory.fit_label_encoder(y_train_balanced)\n",
    "\n",
    "# Encode labels for models that require numeric labels\n",
    "y_train_encoded = factory.transform_labels(y_train_balanced)\n",
    "y_test_encoded = factory.transform_labels(y_test)\n",
    "\n",
    "# Train all models\n",
    "results = {}\n",
    "models_to_train = config['modeling']['models']\n",
    "\n",
    "print(f\"\\nTraining {len(models_to_train)} models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    print(f\"\\n{'='*20} Training {model_name.upper()} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        # Get model configuration\n",
    "        model_config = factory.get_model_config(model_name)\n",
    "        \n",
    "        # Use encoded labels for XGBoost and LightGBM\n",
    "        # if model_name in ['xgboost', 'lightgbm']:\n",
    "        y_train_to_use = y_train_encoded\n",
    "        y_test_to_use = y_test_encoded\n",
    "        # else:\n",
    "        #     y_train_to_use = y_train_balanced\n",
    "        #     y_test_to_use = y_test\n",
    "        \n",
    "        # Train model with cross-validation\n",
    "        training_result = train_model_with_cv(\n",
    "            model_name, \n",
    "            model_config, \n",
    "            X_train_balanced, \n",
    "            y_train_to_use,\n",
    "            cv_folds=config['modeling']['cv_folds'],\n",
    "            n_jobs=config['hyperparameter_tuning']['n_jobs']\n",
    "        )\n",
    "        \n",
    "        if training_result[\"status\"] == \"success\":\n",
    "            # Evaluate model\n",
    "            metrics = evaluate_model(\n",
    "                training_result[\"model\"], \n",
    "                X_train_balanced, y_train_to_use, \n",
    "                X_test, y_test_to_use, \n",
    "                classification_type\n",
    "            )\n",
    "            \n",
    "            # Convert predictions back to original labels if needed\n",
    "            if model_name in ['xgboost', 'lightgbm']:\n",
    "                metrics[\"y_train_pred\"] = factory.inverse_transform_labels(metrics[\"y_train_pred\"])\n",
    "                metrics[\"y_test_pred\"] = factory.inverse_transform_labels(metrics[\"y_test_pred\"])\n",
    "            \n",
    "            results[model_name] = {\n",
    "                **training_result,\n",
    "                **metrics\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ“ {model_name} trained successfully\")\n",
    "            print(f\"  Best CV score: {training_result['best_score']:.3f}\")\n",
    "            print(f\"  Test F1: {metrics['test_f1']:.3f}\")\n",
    "            print(f\"  Test precision: {metrics['test_precision']:.3f}\")\n",
    "            print(f\"  Test recall: {metrics['test_recall']:.3f}\")\n",
    "            print(f\"  Test accuracy: {metrics['test_accuracy']:.3f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"âœ— {model_name} failed: {training_result['error']}\")\n",
    "            results[model_name] = training_result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Unexpected error training {model_name}: {str(e)}\")\n",
    "        results[model_name] = {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "print(f\"\\nTraining completed! {len([r for r in results.values() if r.get('status') == 'success'])} models trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics Tracking\n",
    "\n",
    "For each successfully trained model, we calculate:\n",
    "\n",
    "- Cross-validation scores: Measure model stability\n",
    "- Test set metrics: Evaluate generalization\n",
    "  - Accuracy: Overall correctness\n",
    "  - Precision: False positive control\n",
    "  - Recall: True positive detection\n",
    "  - F1 Score: Balance between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "successful_models = {name: result for name, result in results.items() \n",
    "                    if result.get('status') == 'success'}\n",
    "\n",
    "if successful_models:\n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for model_name, result in successful_models.items():\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'CV Score': f\"{result['best_score']:.3f}\",\n",
    "            'Test Accuracy': f\"{result['test_accuracy']:.3f}\",\n",
    "            'Test F1': f\"{result['test_f1']:.3f}\",\n",
    "            'Test Precision': f\"{result['test_precision']:.3f}\",\n",
    "            'Test Recall': f\"{result['test_recall']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(successful_models.keys(), \n",
    "                         key=lambda x: successful_models[x]['test_f1'])\n",
    "    print(f\"\\nBest model by F1 score: {best_model_name}\")\n",
    "    print(f\"Best F1 score: {successful_models[best_model_name]['test_f1']:.3f}\")\n",
    "else:\n",
    "    print(\"No models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Performance Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"Plot comparison of different metrics across models\"\"\"\n",
    "    metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for model_name, result in results.items():\n",
    "        if result.get('status') == 'success':\n",
    "            for metric, metric_name in zip(metrics, metric_names):\n",
    "                plot_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Metric': metric_name,\n",
    "                    'Score': result[metric]\n",
    "                })\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    df = pd.DataFrame(plot_data)\n",
    "    sns.barplot(data=df, x='Model', y='Score', hue='Metric')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    # Move legend to the right\n",
    "    ax.legend(title=\"Metric\", bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_model_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Confusion Matrices\n",
    "\n",
    "For each model, the confusion matrix shows:\n",
    "\n",
    "- True Positives (TP): Correct positive predictions (diagonal)\n",
    "- False Positives (FP): Incorrect positive predictions (columns)\n",
    "- False Negatives (FN): Missed positive cases (rows)\n",
    "- True Negatives (TN): Correct negative predictions\n",
    "\n",
    "Color intensity indicates:\n",
    "\n",
    "- Darker colors: Higher number of predictions\n",
    "- Lighter colors: Fewer predictions\n",
    "\n",
    "Key insights:\n",
    "\n",
    "- Strong diagonal: Good overall performance\n",
    "- Off-diagonal patterns: Systematic misclassifications\n",
    "- Empty cells: No predictions for that combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Confusion Matrices for Each Model\n",
    "def plot_confusion_matrices(results, y_test):\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    class_labels = sorted(y_test.unique())\n",
    "    successful_results = [(name, res) for name, res in results.items() if res.get('status') == 'success']\n",
    "    n_models = len(successful_results)\n",
    "\n",
    "    fig_cols = min(3, n_models)\n",
    "    fig_rows = (n_models - 1) // fig_cols + 1\n",
    "\n",
    "    fig, axes = plt.subplots(fig_rows, fig_cols, \n",
    "                             figsize=(4 * fig_cols, 4 * fig_rows),\n",
    "                             dpi=config['visualization']['dpi'])\n",
    "\n",
    "    if isinstance(axes, plt.Axes):  # Only one plot\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, (model_name, result) in enumerate(successful_results):\n",
    "        # Convert numeric predictions to string labels\n",
    "        y_pred = result['y_test_pred']\n",
    "        if np.issubdtype(np.array(y_pred).dtype, np.integer):\n",
    "            # Build mapping from class index to label\n",
    "            label_encoder = {i: label for i, label in enumerate(class_labels)}\n",
    "            y_pred = [label_encoder[i] for i in y_pred]\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_labels, yticklabels=class_labels,\n",
    "                    ax=axes[idx])\n",
    "        axes[idx].set_title(f'{model_name}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('True')\n",
    "\n",
    "    # Remove any extra axes\n",
    "    for idx in range(n_models, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_confusion_matrices(results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, the confusion matrix shows:\n",
    "\n",
    "- True Positives (TP): Correct positive predictions (diagonal)\n",
    "- False Positives (FP): Incorrect positive predictions (columns)\n",
    "- False Negatives (FN): Missed positive cases (rows)\n",
    "- True Negatives (TN): Correct negative predictions\n",
    "\n",
    "Color intensity indicates:\n",
    "\n",
    "- Darker colors: Higher number of predictions\n",
    "- Lighter colors: Fewer predictions\n",
    "\n",
    "Key insights:\n",
    "\n",
    "- Strong diagonal: Good overall performance\n",
    "- Off-diagonal patterns: Systematic misclassifications\n",
    "- Empty cells: No predictions for that combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Visualize which features contribute most to model predictions. This analysis is available for tree-based models (Random Forest, XGBoost, LightGBM) that provide feature importance scores.\n",
    "\n",
    "The feature importance plot shows:\n",
    "\n",
    "- Top 10 most influential features\n",
    "- Relative importance scores (0-1 scale)\n",
    "- Comparison across different models\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Higher scores indicate stronger influence on predictions\n",
    "- Different models may rank features differently\n",
    "- Features not shown have lower importance scores\n",
    "\n",
    "Note: Feature importance methods vary by model type:\n",
    "\n",
    "- Random Forest: Mean decrease in impurity\n",
    "- XGBoost/LightGBM: Gain-based importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Plot (for supported models)\n",
    "def plot_feature_importance(results, feature_names):\n",
    "    \"\"\"Plot feature importance for models that support it\"\"\"\n",
    "    supported_models = {\n",
    "        name: result for name, result in results.items()\n",
    "        if (result.get('status') == 'success' and \n",
    "            hasattr(result['model'], 'feature_importances_'))\n",
    "    }\n",
    "    \n",
    "    if not supported_models:\n",
    "        print(\"No models with feature importance found.\")\n",
    "        return\n",
    "    \n",
    "    n_models = len(supported_models)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (name, result) in zip(axes, supported_models.items()):\n",
    "        importances = result['model'].feature_importances_\n",
    "        print(f\"{name} feature importances: {importances[:5]}\")\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        sns.barplot(x=importances[indices][:10], \n",
    "                   y=[feature_names[i] for i in indices][:10],\n",
    "                   ax=ax)\n",
    "        ax.set_title(f'{name} Top 10 Features')\n",
    "        ax.set_xlabel('Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_feature_importance(results, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plots reveal:\n",
    "\n",
    "- Which features drive model decisions\n",
    "- Relative strength of feature contributions\n",
    "- Potential feature redundancy\n",
    "- Opportunities for feature selection refinement\n",
    "\n",
    "This information helps:\n",
    "\n",
    "- Validate feature selection decisions\n",
    "- Guide feature engineering efforts\n",
    "- Understand model behavior\n",
    "- Identify potential data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation Score Distribution\n",
    "\n",
    "Visualize the distribution of cross-validation scores across different models using box plots. This helps assess both model performance and stability.\n",
    "\n",
    "The box plot shows for each model:\n",
    "\n",
    "- Median performance (center line)\n",
    "- Interquartile range (box)\n",
    "- Score spread (whiskers)\n",
    "- Potential outliers (points)\n",
    "\n",
    "Key insights:\n",
    "\n",
    "- Higher median = better overall performance\n",
    "- Smaller box = more stable performance\n",
    "- Wide spread = high sensitivity to data splits\n",
    "- Overlapping boxes = statistically similar models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cross-validation Scores Distribution\n",
    "def plot_cv_scores_distribution(results):\n",
    "    \"\"\"Plot distribution of cross-validation scores\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    cv_scores = []\n",
    "    model_names = []\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if result.get('status') == 'success':\n",
    "            scores = result['cv_results']['mean_test_score']\n",
    "            cv_scores.extend(scores)\n",
    "            model_names.extend([model_name] * len(scores))\n",
    "    \n",
    "    sns.boxplot(x=model_names, y=cv_scores)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Cross-validation Scores Distribution')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('CV Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_cv_scores_distribution(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization helps us understand:\n",
    "\n",
    "- Which models perform most consistently\n",
    "- Performance variability across data splits\n",
    "- Potential overfitting (high variance)\n",
    "- Relative model stability\n",
    "\n",
    "This information guides:\n",
    "\n",
    "- Model selection decisions\n",
    "- Reliability assessment\n",
    "- Potential ensemble strategies\n",
    "- Performance expectations in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Confidence Analysis\n",
    "\n",
    "Examine how confident each model is in its predictions by comparing probability distributions between correct and incorrect predictions. This analysis helps understand model reliability and potential calibration needs.\n",
    "\n",
    "The histograms show:\n",
    "\n",
    "- Distribution of prediction probabilities\n",
    "- Separate views for correct vs. incorrect predictions\n",
    "- Confidence patterns for each model\n",
    "- Potential overconfidence or uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prediction Confidence Analysis\n",
    "def plot_prediction_confidence(results, y_test):\n",
    "    \"\"\"Plot prediction confidence distribution for correct vs incorrect predictions\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(results.items()):\n",
    "        if (result.get('status') == 'success' and \n",
    "            hasattr(result['model'], 'predict_proba')):\n",
    "            \n",
    "            # Get prediction probabilities\n",
    "            y_pred_proba = result['model'].predict_proba(X_test)\n",
    "            confidence = np.max(y_pred_proba, axis=1)\n",
    "            \n",
    "            # Separate correct and incorrect predictions\n",
    "            correct = result['y_test_pred'] == y_test\n",
    "            \n",
    "            # Plot distributions\n",
    "            sns.histplot(data=confidence[correct], \n",
    "                        label='Correct', alpha=0.5, ax=axes[idx])\n",
    "            sns.histplot(data=confidence[~correct], \n",
    "                        label='Incorrect', alpha=0.5, ax=axes[idx])\n",
    "            \n",
    "            axes[idx].set_title(f'{model_name} Prediction Confidence')\n",
    "            axes[idx].set_xlabel('Confidence')\n",
    "            axes[idx].set_ylabel('Count')\n",
    "            axes[idx].legend()\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(len(results), 6):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_prediction_confidence(results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, we can observe:\n",
    "\n",
    "- Ideal pattern: High confidence for correct predictions, low confidence for incorrect ones\n",
    "- Overconfidence: High confidence even for wrong predictions\n",
    "- Uncertainty: Low confidence even for correct predictions\n",
    "\n",
    "Key insights:\n",
    "\n",
    "- Well-calibrated models: Clear separation between correct and incorrect confidence distributions\n",
    "- Overconfident models: High confidence peaks for both distributions\n",
    "- Uncertain models: Overlapping distributions with lower confidence\n",
    "\n",
    "This information helps:\n",
    "\n",
    "- Set appropriate confidence thresholds\n",
    "- Identify need for model calibration\n",
    "- Understand prediction reliability\n",
    "- Guide deployment decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves Analysis\n",
    "\n",
    "Learning curves show how model performance changes with increasing training data. This helps diagnose overfitting, underfitting, and whether more data would help improve performance.\n",
    "\n",
    "The plots show for each model:\n",
    "\n",
    "- Training score (blue line)\n",
    "- Cross-validation score (orange line)\n",
    "- Score variability (shaded regions)\n",
    "- Performance across different training set sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Learning Curves\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves(results, X_train, y_train):\n",
    "    \"\"\"Plot learning curves for all models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(results.items()):\n",
    "        if result.get('status') == 'success':\n",
    "            train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "                result['model'], X_train, y_train,\n",
    "                train_sizes=train_sizes,\n",
    "                cv=5, n_jobs=-1)\n",
    "            \n",
    "            train_mean = np.mean(train_scores, axis=1)\n",
    "            train_std = np.std(train_scores, axis=1)\n",
    "            val_mean = np.mean(val_scores, axis=1)\n",
    "            val_std = np.std(val_scores, axis=1)\n",
    "            \n",
    "            axes[idx].plot(train_sizes_abs, train_mean, \n",
    "                          label='Training score')\n",
    "            axes[idx].fill_between(train_sizes_abs, \n",
    "                                 train_mean - train_std,\n",
    "                                 train_mean + train_std, alpha=0.1)\n",
    "            axes[idx].plot(train_sizes_abs, val_mean, \n",
    "                          label='Cross-validation score')\n",
    "            axes[idx].fill_between(train_sizes_abs,\n",
    "                                 val_mean - val_std,\n",
    "                                 val_mean + val_std, alpha=0.1)\n",
    "            \n",
    "            axes[idx].set_title(f'{model_name} Learning Curve')\n",
    "            axes[idx].set_xlabel('Training Examples')\n",
    "            axes[idx].set_ylabel('Score')\n",
    "            axes[idx].legend(loc='best')\n",
    "            axes[idx].grid(True)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(len(results), 6):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_learning_curves(results, X_train_balanced, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common patterns and their meanings:\n",
    "\n",
    "1. Convergence Gap:\n",
    "   - Small gap: Good fit\n",
    "   - Large gap: Overfitting\n",
    "   - No gap but low scores: Underfitting\n",
    "\n",
    "2. Score Trends:\n",
    "   - Both curves plateau: More data won't help\n",
    "   - Curves still rising: More data might help\n",
    "   - High variance in shaded areas: Unstable performance\n",
    "\n",
    "3. Training Size Impact:\n",
    "   - Rapid initial improvement: Model learns key patterns quickly\n",
    "   - Slow, steady improvement: Complex patterns require more data\n",
    "   - Early plateau: Model reaches capacity quickly\n",
    "\n",
    "This visualization helps:\n",
    "\n",
    "- Determine if more training data would help\n",
    "- Identify overfitting/underfitting\n",
    "- Compare model learning efficiency\n",
    "- Guide data collection efforts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Visualization\n",
    "\n",
    "Visualize model performance using ROC curves:\n",
    "\n",
    "- Plot test set performance for each model\n",
    "- Show micro-averaged ROC for multiclass problems\n",
    "- Display individual class performance when applicable\n",
    "- Include cross-validation scores for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_roc_curves(results, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot ROC curves using the best models from cross-validation.\n",
    "    Uses test set for final evaluation.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing trained models and their results\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = np.unique(y_test)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_models = len([r for r in results.values() if r.get('status') == 'success'])\n",
    "    n_cols = min(3, n_models)\n",
    "    n_rows = (n_models - 1) // n_cols + 1\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 5 * n_rows))\n",
    "    if isinstance(axes, plt.Axes):\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(results.items()):\n",
    "        if result.get('status') != 'success':\n",
    "            continue\n",
    "\n",
    "        ax = axes[idx]\n",
    "        model = result['model']\n",
    "        \n",
    "        if n_classes == 2:\n",
    "            # Binary classification\n",
    "            y_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Explicitly set the positive label to avoid ValueError\n",
    "            positive_label = [label for label in classes if label != classes[0]][0]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=positive_label)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            ax.plot(fpr, tpr, color='b',\n",
    "                    label=f'ROC (AUC = {roc_auc:.2f})',\n",
    "                    lw=2)\n",
    "            \n",
    "        else:\n",
    "            # Multiclass classification\n",
    "            y_test_bin = label_binarize(y_test, classes=classes)\n",
    "            y_score = model.predict_proba(X_test)\n",
    "\n",
    "            # Micro-average ROC curve and AUC\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            ax.plot(fpr, tpr, color='b',\n",
    "                    label=f'Micro-avg ROC (AUC = {roc_auc:.2f})',\n",
    "                    lw=2)\n",
    "\n",
    "            # Per-class ROC curves (optional)\n",
    "            for i in range(n_classes):\n",
    "                fpr_class, tpr_class, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "                auc_class = auc(fpr_class, tpr_class)\n",
    "                ax.plot(fpr_class, tpr_class, lw=1, alpha=0.3,\n",
    "                        label=f'Class {classes[i]} (AUC = {auc_class:.2f})')\n",
    "        \n",
    "        # Plot random chance line\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "                alpha=.8, label='Random chance')\n",
    "        \n",
    "        # Show cross-validation score if available\n",
    "        if 'best_score' in result:\n",
    "            cv_text = f'CV Score: {result[\"best_score\"]:.2f}'\n",
    "            if 'cv_results_' in result and 'mean_test_score' in result['cv_results_']:\n",
    "                cv_std = np.std(result['cv_results_']['mean_test_score'])\n",
    "                cv_text += f' Â± {cv_std:.2f}'\n",
    "            ax.text(0.05, 0.95, cv_text,\n",
    "                    transform=ax.transAxes, fontsize=8,\n",
    "                    verticalalignment='top')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_title(f'{model_name} ROC Curve\\n{\"Binary\" if n_classes == 2 else \"Multiclass\"} Classification')\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.legend(loc='lower right', fontsize='small')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(n_models, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_roc_curves(results, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization helps us understand:\n",
    "\n",
    "- Which models perform best for our specific task\n",
    "- How well models generalize to unseen data\n",
    "- Any class-specific performance patterns\n",
    "- Stability of model performance across CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "save_dir = Path('../data/processed')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Timestamp for versioning\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Collect objects to save (keep your existing structure)\n",
    "training_artifacts = {\n",
    "    'models': {\n",
    "        name: result['model'] \n",
    "        for name, result in results.items() \n",
    "        if result.get('status') == 'success'\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        name: {\n",
    "            'test_accuracy': result['test_accuracy'],\n",
    "            'test_precision': result['test_precision'],\n",
    "            'test_recall': result['test_recall'],\n",
    "            'test_f1': result['test_f1'],\n",
    "            'cv_score': result['best_score'],\n",
    "            'best_params': result['best_params']\n",
    "        }\n",
    "        for name, result in results.items()\n",
    "        if result.get('status') == 'success'\n",
    "    },\n",
    "    'feature_info': {\n",
    "        'feature_names': selected_features,\n",
    "        'n_features': len(selected_features)\n",
    "    },\n",
    "    'label_info': {\n",
    "        'label_encoder': factory.label_encoder if hasattr(factory, 'label_encoder') else None,\n",
    "        'classes': np.unique(y_train_balanced),\n",
    "        'classification_type': classification_type,\n",
    "        'n_classes': n_classes\n",
    "    },\n",
    "    'training_config': {\n",
    "        'model_configs': {\n",
    "            name: result['best_params']\n",
    "            for name, result in results.items()\n",
    "            if result.get('status') == 'success'\n",
    "        },\n",
    "        'original_config': config\n",
    "    },\n",
    "    'data_info': {\n",
    "        'n_train_samples': len(X_train_balanced),\n",
    "        'n_test_samples': len(X_test),\n",
    "        'feature_names': selected_features,\n",
    "        'training_timestamp': timestamp\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add feature importance information if available\n",
    "for name, model in training_artifacts['models'].items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        if 'feature_importance' not in training_artifacts:\n",
    "            training_artifacts['feature_importance'] = {}\n",
    "        training_artifacts['feature_importance'][name] = {\n",
    "            'importance_scores': model.feature_importances_.tolist(),\n",
    "            'feature_names': selected_features\n",
    "        }\n",
    "\n",
    "# Save artifacts\n",
    "artifacts_file = save_dir / f'05_model_training_artifacts_{timestamp}.pkl'\n",
    "with open(artifacts_file, 'wb') as f:\n",
    "    pickle.dump(training_artifacts, f)\n",
    "\n",
    "# Save summary (keep your existing structure)\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'models_trained': list(training_artifacts['models'].keys()),\n",
    "    'best_model': max(\n",
    "        training_artifacts['performance_metrics'].items(),\n",
    "        key=lambda x: x[1]['test_f1']\n",
    "    )[0],\n",
    "    'feature_count': training_artifacts['feature_info']['n_features'],\n",
    "    'class_count': training_artifacts['label_info']['n_classes'],\n",
    "    'training_samples': training_artifacts['data_info']['n_train_samples'],\n",
    "    'test_samples': training_artifacts['data_info']['n_test_samples']\n",
    "}\n",
    "\n",
    "summary_file = save_dir / f'05_model_training_summary_{timestamp}.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "# Add a manifest file to track the latest version\n",
    "manifest = {\n",
    "    'timestamp': timestamp,\n",
    "    'files': {\n",
    "        'artifacts': str(artifacts_file.name),\n",
    "        'summary': str(summary_file.name)\n",
    "    },\n",
    "    'best_model': summary['best_model'],\n",
    "    'best_model_score': float(training_artifacts['performance_metrics'][summary['best_model']]['test_f1']),\n",
    "    'feature_count': summary['feature_count'],\n",
    "    'class_count': summary['class_count'],\n",
    "    'model_versions': {\n",
    "        name: {\n",
    "            'test_f1': metrics['test_f1'],\n",
    "            'parameters': training_artifacts['training_config']['model_configs'][name]\n",
    "        }\n",
    "        for name, metrics in training_artifacts['performance_metrics'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "manifest_file = save_dir / 'model_manifest.json'\n",
    "with open(manifest_file, 'w') as f:\n",
    "    json.dump(manifest, f, indent=4)\n",
    "\n",
    "print(f\"Saved model training artifacts to: {artifacts_file}\")\n",
    "print(f\"Saved summary to: {summary_file}\")\n",
    "print(f\"Updated manifest: {manifest_file}\")\n",
    "print(\"\\nSaved objects include:\")\n",
    "for key in training_artifacts.keys():\n",
    "    print(f\"- {key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
