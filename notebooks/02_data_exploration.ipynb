{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Data Exploration\"\n",
    "author:\n",
    "  Chao-Chung Kuo\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    toc-expand: 3\n",
    "    toc-location: left\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "    html-table-processing: none\n",
    "    other-links:\n",
    "      - text: Main Report\n",
    "        href: index.html\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This notebook performs comprehensive exploratory data analysis (EDA) and quality assessment on the processed data from the previous step.\n",
    "\n",
    "## Objectives\n",
    "- Assess data quality and identify issues\n",
    "- Explore feature distributions and relationships\n",
    "- Analyze class balance and separability\n",
    "- Generate quality metrics and recommendations\n",
    "- Prepare data for modeling phase\n",
    "\n",
    "All parameters are managed through `config.toml` for reproducibility and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import toml\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Load configuration\n",
    "config = toml.load('../config.toml')\n",
    "sample_id_col = config['data']['sample_id_column']\n",
    "label_col = config['data']['label_column']\n",
    "\n",
    "# Apply visualization settings from config\n",
    "if 'visualization' in config:\n",
    "    if 'dpi' in config['visualization']:\n",
    "        plt.rcParams['figure.dpi'] = config['visualization']['dpi']\n",
    "    if 'figure_size' in config['visualization']:\n",
    "        plt.rcParams['figure.figsize'] = config['visualization']['figure_size']\n",
    "    if 'color_palette' in config['visualization']:\n",
    "        sns.set_palette(config['visualization']['color_palette'])\n",
    "\n",
    "# Display configuration\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Data input: {config['data']['input_path']}\")\n",
    "print(f\"Label column: {config['data']['label_column']}\")\n",
    "print(f\"Outlier method: {config['preprocessing']['outlier_method']}\")\n",
    "print(f\"Visualization DPI: {config['visualization']['dpi']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from previous step\n",
    "data_dir = Path('../data/processed')\n",
    "\n",
    "# Load all pickle files\n",
    "with open(data_dir / '01_data_ingestion.pkl', 'rb') as f:\n",
    "    data_objects = pickle.load(f)\n",
    "\n",
    "# Extract key objects\n",
    "df = data_objects[\"data\"]['df']\n",
    "feature_cols = data_objects[\"data\"]['feature_cols']\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(df)}\")\n",
    "print(f\"Classes: {df[label_col].nunique()}\")\n",
    "print(f\"Label distribution:\\n{df[label_col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "We'll systematically assess data quality across multiple dimensions:\n",
    "- Missing values\n",
    "- Data types and ranges\n",
    "- Duplicates\n",
    "- Outliers\n",
    "- Data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall missing values\n",
    "total_missing = missing_data.sum()\n",
    "total_missing_percent = (total_missing / (len(df) * len(df.columns))) * 100\n",
    "\n",
    "print(f\"Total missing values: {total_missing}\")\n",
    "print(f\"Overall missing percentage: {total_missing_percent:.2f}%\")\n",
    "\n",
    "# Features with missing values\n",
    "features_with_missing = missing_data[missing_data > 0]\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\nFeatures with missing values ({len(features_with_missing)}):\")\n",
    "    for feature, count in features_with_missing.items():\n",
    "        print(f\"  {feature}: {count} ({missing_percent[feature]:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found in any feature.\")\n",
    "\n",
    "# Visualize missing values\n",
    "if total_missing > 0:\n",
    "    plt.figure(figsize=tuple(config['visualization']['figure_size']))\n",
    "    missing_percent.plot(kind='bar')\n",
    "    plt.title('Missing Values by Feature')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Missing Percentage (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and ranges analysis\n",
    "print(\"Data Types and Ranges Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Numeric features summary\n",
    "numeric_features = df[feature_cols].select_dtypes(include=[np.number])\n",
    "print(f\"\\nNumeric features: {len(numeric_features.columns)}\")\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary of Features:\")\n",
    "print(numeric_features.iloc[:, :5].describe())\n",
    "\n",
    "# Check for infinite values\n",
    "infinite_counts = np.isinf(numeric_features).sum()\n",
    "if infinite_counts.sum() > 0:\n",
    "    print(f\"\\nInfinite values found: {infinite_counts.sum()}\")\n",
    "    for col, count in infinite_counts[infinite_counts > 0].items():\n",
    "        print(f\"  {col}: {count}\")\n",
    "else:\n",
    "    print(\"\\nNo infinite values found.\")\n",
    "\n",
    "# Value ranges\n",
    "print(\"\\nValue Ranges:\")\n",
    "for col in feature_cols[:5]:\n",
    "    col_data = df[col].dropna()\n",
    "    if len(col_data) > 0:\n",
    "        print(f\"{col}: [{col_data.min():.3f}, {col_data.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate detection\n",
    "print(\"Duplicate Detection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_percent = (duplicate_rows / len(df)) * 100\n",
    "\n",
    "print(f\"Duplicate rows: {duplicate_rows} ({duplicate_percent:.2f}%)\")\n",
    "\n",
    "# Check for duplicate feature values\n",
    "duplicate_features = []\n",
    "for col in feature_cols:\n",
    "    unique_ratio = df[col].nunique() / len(df)\n",
    "    if unique_ratio < 0.1:  # Less than 10% unique values\n",
    "        duplicate_features.append((col, unique_ratio))\n",
    "\n",
    "if duplicate_features:\n",
    "    print(f\"\\nFeatures with low uniqueness ({len(duplicate_features)}):\")\n",
    "    for feature, ratio in duplicate_features[:5]:\n",
    "        print(f\"  {feature}: {ratio:.3f} unique ratio\")\n",
    "else:\n",
    "    print(\"\\nAll features have good uniqueness.\")\n",
    "\n",
    "# Check for duplicate sample IDs (if present)\n",
    "if sample_id_col and sample_id_col in df.columns:\n",
    "    duplicate_ids = df[sample_id_col].duplicated().sum()\n",
    "    if duplicate_ids > 0:\n",
    "        print(f\"\\nDuplicate sample IDs: {duplicate_ids}\")\n",
    "    else:\n",
    "        print(\"\\nNo duplicate sample IDs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using configured method\n",
    "print(\"Outlier Detection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "outlier_method = config['preprocessing']['outlier_method']\n",
    "print(f\"Using method: {outlier_method}\")\n",
    "\n",
    "# Prepare data for outlier detection\n",
    "X = df[feature_cols].fillna(df[feature_cols].mean())\n",
    "\n",
    "outliers_by_feature = {}\n",
    "total_outliers = 0\n",
    "\n",
    "if outlier_method == \"iqr\":\n",
    "    # IQR method\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        outliers = ((X[col] < lower_bound[col]) | (X[col] > upper_bound[col])).sum()\n",
    "        outliers_by_feature[col] = outliers\n",
    "        total_outliers += outliers\n",
    "\n",
    "elif outlier_method == \"zscore\":\n",
    "    # Z-score method\n",
    "    z_scores = np.abs(stats.zscore(X))\n",
    "    threshold = 3.0\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        outliers = (z_scores[:, feature_cols.index(col)] > threshold).sum()\n",
    "        outliers_by_feature[col] = outliers\n",
    "        total_outliers += outliers\n",
    "\n",
    "elif outlier_method == \"isolation_forest\":\n",
    "    # Isolation Forest method\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outlier_labels = iso_forest.fit_predict(X)\n",
    "    outliers = (outlier_labels == -1).sum()\n",
    "    total_outliers = outliers\n",
    "    outliers_by_feature = {\"Overall\": outliers}\n",
    "\n",
    "print(f\"Total outliers detected: {total_outliers}\")\n",
    "print(f\"Outlier percentage: {(total_outliers / (len(df) * len(feature_cols))) * 100:.2f}%\")\n",
    "\n",
    "# Show features with most outliers\n",
    "if outlier_method != \"isolation_forest\":\n",
    "    top_outlier_features = sorted(outliers_by_feature.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"\\nTop 10 features with most outliers:\")\n",
    "    for feature, count in top_outlier_features:\n",
    "        if count > 0:\n",
    "            print(f\"  {feature}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Feature Analysis\n",
    "\n",
    "We'll analyze feature distributions, correlations, and importance to understand the data structure and identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"Correlation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[feature_cols].corr()\n",
    "\n",
    "# Find highly correlated features\n",
    "threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > threshold:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "print(f\"Features with correlation > {threshold}: {len(high_corr_pairs)}\")\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "        print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "\n",
    "# Visualize correlation matrix (top features)\n",
    "n_corr_features = min(20, len(feature_cols))\n",
    "top_features = feature_cols[:n_corr_features]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix.loc[top_features, top_features], \n",
    "            annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title(f'Correlation Matrix (Top {n_corr_features} Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature variance analysis\n",
    "print(\"\\nFeature Variance Analysis:\")\n",
    "variances = df[feature_cols].var().sort_values(ascending=False)\n",
    "print(\"Top 10 features by variance:\")\n",
    "for feature, var in variances.head(10).items():\n",
    "    print(f\"  {feature}: {var:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Analysis\n",
    "\n",
    "We'll analyze the target variable distribution and explore how features relate to different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution analysis\n",
    "print(\"Label Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Class distribution\n",
    "class_counts = df[label_col].value_counts()\n",
    "class_percentages = (class_counts / len(df)) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"  {class_name}: {count} ({class_percentages[class_name]:.1f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=tuple(config['visualization']['figure_size']))\n",
    "plt.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Class balance assessment\n",
    "n_classes = len(class_counts)\n",
    "min_class_size = class_counts.min()\n",
    "max_class_size = class_counts.max()\n",
    "balance_ratio = min_class_size / max_class_size\n",
    "\n",
    "print(f\"\\nClass Balance Assessment:\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Smallest class: {min_class_size}\")\n",
    "print(f\"Largest class: {max_class_size}\")\n",
    "print(f\"Balance ratio: {balance_ratio:.3f}\")\n",
    "\n",
    "if balance_ratio < 0.3:\n",
    "    print(\"⚠️  Severe class imbalance detected!\")\n",
    "elif balance_ratio < 0.5:\n",
    "    print(\"⚠️  Moderate class imbalance detected.\")\n",
    "else:\n",
    "    print(\"✓ Classes are reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nSelecting top features based on class-wise variation...\")\n",
    "\n",
    "# Drop rows with NaNs in features\n",
    "X = df[feature_cols].dropna()\n",
    "y = df.loc[X.index, label_col]\n",
    "\n",
    "# Compute F-statistics for each feature\n",
    "f_scores, _ = f_classif(X, y)\n",
    "\n",
    "# Rank features by F-score\n",
    "top_indices = np.argsort(f_scores)[::-1]  # descending order\n",
    "top_features = [feature_cols[i] for i in top_indices[:10]]\n",
    "\n",
    "print(\"Top 10 informative features by ANOVA F-score:\")\n",
    "for i, f in enumerate(top_features):\n",
    "    print(f\"  {i+1}. {f} (F-score = {f_scores[feature_cols.index(f)]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    for class_name in df[label_col].unique():\n",
    "        class_data = df[df[label_col] == class_name][feature].dropna()\n",
    "        axes[i].hist(class_data, alpha=0.6, label=class_name, bins=20)\n",
    "    \n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report\n",
    "\n",
    "Based on our comprehensive analysis, here's a summary of data quality findings and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate quality report\n",
    "print(\"Data Quality Report:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate quality metrics\n",
    "quality_metrics = {\n",
    "    'missing_values_percent': total_missing_percent,\n",
    "    'duplicate_rows_percent': duplicate_percent,\n",
    "    'outlier_percent': (total_outliers / (len(df) * len(feature_cols))) * 100,\n",
    "    'class_balance_ratio': balance_ratio,\n",
    "    'high_correlation_pairs': len(high_corr_pairs),\n",
    "    'features_with_low_uniqueness': len(duplicate_features)\n",
    "}\n",
    "\n",
    "print(\"Quality Metrics:\")\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = []\n",
    "\n",
    "if quality_metrics['missing_values_percent'] > 5:\n",
    "    recommendations.append(\"Consider imputation strategy for missing values\")\n",
    "    \n",
    "if quality_metrics['duplicate_rows_percent'] > 1:\n",
    "    recommendations.append(\"Remove or investigate duplicate rows\")\n",
    "    \n",
    "if quality_metrics['outlier_percent'] > 10:\n",
    "    recommendations.append(\"Consider outlier treatment strategy\")\n",
    "    \n",
    "if quality_metrics['class_balance_ratio'] < 0.3:\n",
    "    recommendations.append(\"Use class balancing techniques (SMOTE, class weights)\")\n",
    "    \n",
    "if quality_metrics['high_correlation_pairs'] > 10:\n",
    "    recommendations.append(\"Consider feature selection to remove highly correlated features\")\n",
    "    \n",
    "if quality_metrics['features_with_low_uniqueness'] > 0:\n",
    "    recommendations.append(\"Investigate features with low uniqueness\")\n",
    "\n",
    "print(f\"\\nRecommendations ({len(recommendations)}):\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "# Overall quality score\n",
    "quality_score = 100\n",
    "if quality_metrics['missing_values_percent'] > 5:\n",
    "    quality_score -= 20\n",
    "if quality_metrics['duplicate_rows_percent'] > 1:\n",
    "    quality_score -= 15\n",
    "if quality_metrics['outlier_percent'] > 10:\n",
    "    quality_score -= 15\n",
    "if quality_metrics['class_balance_ratio'] < 0.3:\n",
    "    quality_score -= 10\n",
    "if quality_metrics['high_correlation_pairs'] > 10:\n",
    "    quality_score -= 10\n",
    "\n",
    "print(f\"\\nOverall Data Quality Score: {quality_score}/100\")\n",
    "if quality_score >= 80:\n",
    "    print(\"✓ Excellent data quality\")\n",
    "elif quality_score >= 60:\n",
    "    print(\"✓ Good data quality\")\n",
    "elif quality_score >= 40:\n",
    "    print(\"⚠️  Moderate data quality issues\")\n",
    "else:\n",
    "    print(\"⚠️  Significant data quality issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Analysis Results\n",
    "\n",
    "We'll save the analysis results and quality assessment for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prepare results to save\n",
    "eda_results = {\n",
    "    'quality_metrics': quality_metrics,\n",
    "    'recommendations': recommendations,\n",
    "    'quality_score': quality_score,\n",
    "    'correlation_matrix': correlation_matrix,\n",
    "    'high_corr_pairs': high_corr_pairs,\n",
    "    'outliers_by_feature': outliers_by_feature,\n",
    "    'class_distribution': class_counts.to_dict(),\n",
    "    'feature_variances': variances.to_dict(),\n",
    "    'analysis_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(output_dir / '02_eda_results.pkl', 'wb') as f:\n",
    "    pickle.dump(eda_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
