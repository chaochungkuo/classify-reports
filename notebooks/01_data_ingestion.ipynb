{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Data Ingestion and Validation\"\n",
    "author:\n",
    "  Chao-Chung Kuo\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    toc-expand: 3\n",
    "    toc-location: left\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "    html-table-processing: none\n",
    "    other-links:\n",
    "      - text: Main Report\n",
    "        href: index.html\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This notebook covers:\n",
    "\n",
    "1. Loading tabular data with pandas\n",
    "2. Data validation and quality checks\n",
    "3. Basic data exploration and statistics\n",
    "4. Data visualization and relationships\n",
    "\n",
    "The data format follows these requirements:\n",
    "- Each row represents a unique sample with a distinct sample_id\n",
    "- Features are numeric values used for classification\n",
    "- Labels represent classification groups (can be repeated across samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import toml\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load configuration from a TOML file.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the TOML configuration file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the configuration parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = toml.load(config_path)\n",
    "        print(\"Configuration loaded successfully!\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading configuration: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(Path('../config.toml'))\n",
    "\n",
    "# Apply visualization settings from config\n",
    "if 'visualization' in config:\n",
    "    if 'dpi' in config['visualization']:\n",
    "        plt.rcParams['figure.dpi'] = config['visualization']['dpi']\n",
    "    if 'figure_size' in config['visualization']:\n",
    "        plt.rcParams['figure.figsize'] = config['visualization']['figure_size']\n",
    "    if 'color_palette' in config['visualization']:\n",
    "        sns.set_palette(config['visualization']['color_palette'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Load Data\n",
    "\n",
    "The data structure follows these rules:\n",
    "- Each row represents a unique sample with a distinct `sample_id`\n",
    "- Features are numeric values for classification\n",
    "- `labels` can be repeated as they represent classification groups\n",
    "\n",
    "Let's load and examine this data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data using config\n",
    "data_config = config['data']\n",
    "data_path = data_config['input_path']\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Get column names from config\n",
    "sample_id_col = data_config['sample_id_column']\n",
    "label_col = data_config['label_column']\n",
    "\n",
    "# Get feature columns\n",
    "if data_config['feature_columns']:\n",
    "    # Use explicitly specified feature columns\n",
    "    feature_cols = data_config['feature_columns']\n",
    "else:\n",
    "    # Auto-detect feature columns (all numeric columns except sample_id and label)\n",
    "    feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if sample_id_col in feature_cols:\n",
    "        feature_cols.index = df[sample_id_col]\n",
    "        feature_cols.remove(sample_id_col)\n",
    "    if label_col in feature_cols:\n",
    "        feature_cols.remove(label_col)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Feature names: {feature_cols}\")\n",
    "print(f\"Number of unique samples: {df[sample_id_col].nunique()}\")\n",
    "print(f\"Unique labels: {df[label_col].unique()}\")\n",
    "print(f\"Label distribution: {df[label_col].value_counts().to_dict()}\")\n",
    "\n",
    "# Basic data validation\n",
    "print(\"\\nData Validation:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check for duplicate sample IDs\n",
    "if len(df) != df[sample_id_col].nunique():\n",
    "    print(\"\\nWARNING: Duplicate sample IDs found!\")\n",
    "    duplicates = df[df[sample_id_col].duplicated(keep=False)]\n",
    "    print(duplicates[[sample_id_col, label_col]])\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.any():\n",
    "    print(\"\\nWARNING: Missing values found:\")\n",
    "    print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['preprocessing']['log_transform'] == \"log2\":\n",
    "    df[feature_cols] = np.log2(df[feature_cols]+1)\n",
    "    print(\"Log2 transformation applied\")\n",
    "elif config['preprocessing']['log_transform'] == \"log10\":\n",
    "    df[feature_cols] = np.log10(df[feature_cols]+1)\n",
    "    print(\"Log10 transformation applied\")\n",
    "elif config['preprocessing']['log_transform'] == \"ln\":\n",
    "    df[feature_cols] = np.log(df[feature_cols]+1)\n",
    "    print(\"Natural logarithm transformation applied\")\n",
    "else:\n",
    "    print(\"No log transformation applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Validation\n",
    "\n",
    "Let's perform some basic validation checks on the input data:\n",
    "1. Check for missing values\n",
    "2. Verify data types (numeric features, categorical labels)\n",
    "3. Check value ranges and distributions\n",
    "4. Validate label categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for missing values\n",
    "print(\"Missing Values Check:\")\n",
    "print(\"-\" * 40)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2. Verify data types\n",
    "print(\"\\nData Types Check:\")\n",
    "print(\"-\" * 40)\n",
    "print(df.dtypes)\n",
    "\n",
    "# 3. Validate sample_ids\n",
    "print(\"\\nSample ID Validation:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of sample_ids: {len(df['sample_id'])}\")\n",
    "print(f\"Number of unique sample_ids: {df['sample_id'].nunique()}\")\n",
    "if len(df['sample_id']) != df['sample_id'].nunique():\n",
    "    print(\"WARNING: Duplicate sample_ids found!\")\n",
    "    print(\"\\nDuplicate sample_ids:\")\n",
    "    print(df['sample_id'].value_counts()[df['sample_id'].value_counts() > 1])\n",
    "\n",
    "# 4. Check value ranges for numeric features\n",
    "print(\"\\nValue Ranges for Features:\")\n",
    "print(\"-\" * 40)\n",
    "for col in df.columns[:5]:  # Exclude sample_id and label columns\n",
    "    if col not in [sample_id_col, label_col]:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Min: {df[col].min():.3f}\")\n",
    "        print(f\"  Max: {df[col].max():.3f}\")\n",
    "        print(f\"  Mean: {df[col].mean():.3f}\")\n",
    "        print(f\"  Std: {df[col].std():.3f}\")\n",
    "\n",
    "# 5. Validate labels\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Label counts:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nSamples per label:\")\n",
    "for label in df['label'].unique():\n",
    "    print(f\"\\nLabel {label}:\")\n",
    "    print(df[df['label'] == label]['sample_id'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default visualization parameters\n",
    "default_fig_size = (15, 10)\n",
    "default_color_palette = 'viridis'\n",
    "\n",
    "# Get visualization settings from config if available\n",
    "viz_config = config.get('visualization', {})\n",
    "fig_size = viz_config.get('figure_size', default_fig_size)\n",
    "color_palette = viz_config.get('color_palette', default_color_palette)\n",
    "\n",
    "# Set color palette\n",
    "sns.set_palette(color_palette)\n",
    "\n",
    "# Function to select top features by variance\n",
    "def select_top_features(data, feature_cols, n=100):\n",
    "    \"\"\"Select top N features by variance.\"\"\"\n",
    "    variances = data[feature_cols].var().sort_values(ascending=False)\n",
    "    return variances.head(n).index.tolist()\n",
    "\n",
    "# Function to create feature correlation matrix\n",
    "def compute_feature_correlations(data, features):\n",
    "    \"\"\"Compute correlation matrix for selected features.\"\"\"\n",
    "    return data[features].corr()\n",
    "\n",
    "# 1. Label Distribution Analysis\n",
    "plt.figure(figsize=fig_size)\n",
    "plt.title('Label Distribution', pad=20, fontsize=14)\n",
    "sns.countplot(data=df, x=label_col)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "total = len(df)\n",
    "for p in plt.gca().patches:\n",
    "    percentage = f'{100 * p.get_height() / total:.1f}%'\n",
    "    plt.annotate(percentage, (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Distribution Heatmap\n",
    "print(\"\\nGenerating Feature Distribution Heatmap...\")\n",
    "# Select top features by variance if there are too many\n",
    "n_features = len(feature_cols)\n",
    "if n_features > 100:\n",
    "    print(f\"Selecting top 100 features by variance out of {n_features} total features\")\n",
    "    selected_features = select_top_features(df, feature_cols, n=100)\n",
    "else:\n",
    "    selected_features = feature_cols\n",
    "\n",
    "# Create feature distribution heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Feature Distribution Heatmap (Normalized Values)', pad=20, fontsize=14)\n",
    "\n",
    "# Normalize and plot feature values\n",
    "feature_data = df[selected_features].apply(lambda x: (x - x.mean()) / x.std())\n",
    "sns.heatmap(feature_data.T, cmap='coolwarm', center=0, \n",
    "            yticklabels=True, xticklabels=False)\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Correlation Analysis\n",
    "print(\"\\nGenerating Feature Correlation Analysis...\")\n",
    "# Compute correlations for selected features\n",
    "correlation_matrix = compute_feature_correlations(df, selected_features)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Feature Correlation Heatmap', pad=20, fontsize=14)\n",
    "mask = np.triu(np.ones_like(correlation_matrix), k=1)\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PCA Analysis\n",
    "print(\"\\nPerforming PCA Analysis...\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Prepare data for PCA\n",
    "# Impute missing values before scaling and PCA\n",
    "imputer = SimpleImputer(strategy=\"mean\")  # or \"median\", etc.\n",
    "X_imputed = imputer.fit_transform(df[feature_cols])\n",
    "X_scaled = StandardScaler().fit_transform(X_imputed)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create PCA plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('PCA of Features by Label', pad=20, fontsize=14)\n",
    "\n",
    "# Create scatter plot with labels\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=pd.factorize(df[label_col])[0],\n",
    "                     cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Add legend\n",
    "unique_labels = df[label_col].unique()\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                            markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "                            label=label, markersize=10)\n",
    "                  for i, label in enumerate(unique_labels)]\n",
    "plt.legend(handles=legend_elements, title='Labels')\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance summary\n",
    "print(\"\\nTop Features by Contribution to Principal Components:\")\n",
    "feature_importance = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=feature_cols\n",
    ")\n",
    "feature_importance['total_importance'] = np.abs(feature_importance).sum(axis=1)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.sort_values('total_importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data and models for next steps\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory for saved objects if it doesn't exist\n",
    "save_dir = Path('../data/processed')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dictionary to store all objects we want to save\n",
    "objects_to_save = {\n",
    "    'data': {\n",
    "        'df': df,  # Original dataframe\n",
    "        'feature_cols': feature_cols  # List of feature columns\n",
    "    },\n",
    "    'metadata': {\n",
    "        'sample_id_col': sample_id_col,\n",
    "        'label_col': label_col,\n",
    "        'n_features': len(feature_cols),\n",
    "        'n_samples': len(df),\n",
    "        'label_distribution': df[label_col].value_counts().to_dict()\n",
    "    }\n",
    "}\n",
    "\n",
    "output_file = save_dir / f'01_data_ingestion.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(objects_to_save, f)\n",
    "print(f\"Saved objects to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
